---
title: "Credit Card Fraud Detection"
output:
  rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup
```{r}
credit = read.csv("./creditcard.csv")
```

## Examine Data
```{r}
table(credit$Class)
table(credit$Class)/nrow(credit)
```
The given data is heavily skewed, with only 0.172% of fraud cases. We have to deal with this imbalance.

## Preprocess
```{r}
# scale Time and Amount
credit$Time = as.numeric(scale(credit$Time))
credit$Amount = as.numeric(scale(credit$Amount))
```

## Split Data
```{r}
nfraud = subset(credit, Class == "0")
fraud = subset(credit, Class == "1")

set.seed(99)
nfraud_index = sample(nrow(nfraud), 0.7 * nrow(nfraud), replace = FALSE)
fraud_index = sample(nrow(fraud), 0.7 * nrow(fraud), replace = FALSE)

nfraud_train = nfraud[nfraud_index, ]
nfraud_test = nfraud[-nfraud_index, ]
fraud_train = fraud[fraud_index, ]
fraud_test = fraud[-fraud_index, ]

train = rbind(nfraud_train, fraud_train)
test = rbind(nfraud_test, fraud_test)
```
Stratified random sampling

## PCA for Test Data
```{r}
pca_test = prcomp(test[, -31])
test_1 = pca_test$x[, 1] # PCA1
test_2 = pca_test$x[, 2] # PCA2
```
The given data includes Amount and Time, which are not part of previously done PCA. For the sake of visualization, we will conduct another PCA including all given variables.

# Undersample
One way to deal with imbalanced data is to undersample. I will randomly choose 492 non-frauds and combine with frauds. This will make new data with size of 984, that has 50:50 distribution between non-frauds and frauds.
```{r}
library(dplyr)
set.seed(11)
nonf = sample_n(credit[credit$Class == 0, ], size = sum(credit$Class == 1))
df = rbind(nonf, credit[credit$Class == 1, ])
```

## Examine corr
The new balanced data will tell differently about the data. Here, let's look at the correlation between each PCA variables and whether fraud.
```{r}
library(corrplot)
corrplot(cor(credit), method = "shade", title = "With original data")
corrplot(cor(df), method = "shade", title = "With balanced data")
```

The original data shows only vague correlations. In contrast, the new balanced data shows much clearer correlations.  
Specifically,  
- V2, V4, and V11 have positive correlation with Class.  
- V3, V9, V10, V12, V14, and V16 have negative correlation with Class.

## Anomaly detection
For the variables found to have strong correlations with Class, let's proceed to find outliers and remove them. This will affect the accuracy performance of prediction.  
For anomaly detection, we will use 1.5 IQR barrier to decide which to keep and drop.
```{r}
new_df = df

anomaly_report = data.frame('Var' = 0, 'Upper' = 0, 'Lower' = 0, 'outliers' = 0)

var_index = c(2, 3, 4, 9, 10, 11, 12, 14, 16)
index = rep(1, nrow(new_df))
for (i in 1:9) {
  q1 = summary(new_df[, var_index[i]+1])[2]
  q3 = summary(new_df[, var_index[i]+1])[5]
  iqr = q3 - q1
  cutoff = iqr * 1.5
  lower = q1 - cutoff
  upper = q3 + cutoff
  
  outlier = new_df[, var_index[i]+1] < lower | new_df[, var_index[i]+1] > upper
  
  index = index * !outlier
  
  anomaly_report[i, 1] = paste("V", var_index[i], sep = "")
  anomaly_report[i, 2] = lower
  anomaly_report[i, 3] = upper
  anomaly_report[i, 4] = sum(outlier)
}

new_df = new_df[as.logical(index), ]

print(anomaly_report)
```

## Clustering
### t-SNE
```{r}
X = new_df[, -31]
y = new_df[, 31]

clus = ifelse(y == 1, 15, 16)

library(Rtsne)
tsne = Rtsne(X, dims = 2, check_duplicates = FALSE, normalize = FALSE)
plot(tsne$Y[, 1], tsne$Y[, 2], pch = clus, col = clus)
legend("topleft", legend = c("Fraud", "Non-fraud"), pch = c(15, 16), col = c(15, 16))
```

On the left side, we can clearly see a cluster, consist only of frauds. However, on the right side, frauds and non-frauds are mixed together.

### PCA
```{r}
pca = prcomp(new_df[, -31])

plot(pca$x[, 1], pca$x[, 2], pch = clus, col = clus)
legend("bottomleft", legend = c("Fraud", "Non-fraud"), pch = c(15, 16), col = c(15, 16))
```

### FA
```{r}
fa = factanal(new_df[, -31], 2, rotation = "varimax")

fa_df = as.matrix(new_df[, -31]) %*% as.matrix(fa$loadings)

plot(fa_df[, 1], -fa_df[, 2], pch = clus, col = clus)
legend("topleft", legend = c("Fraud", "Non-fraud"), pch = c(15, 16), col = c(15, 16))
```

PCA and FA show similar visualization, possibly related to t-SNE. We can see fraud cases on the left, distinct from non-frauds. Yet, on the right side, non-frauds are mixed with frauds, hard to distiguish from each other.

## Classifier
### Logistic Regression
```{r}
lr = glm(Class ~ ., new_df, family = "binomial")

pred_lr = predict(lr, newdata = test, type = "response")
lr_index = as.numeric(pred_lr > 0.9) # cutoff

plot(test_1, test_2, pch = lr_index+15, col = lr_index+15)
legend("topleft", legend = c("Fraud", "Non-fraud"), pch = c(15, 16), col = c(15, 16))

library(caret)
lr_confmat = confusionMatrix(as.factor(lr_index), as.factor(test$Class), positive = "1")
lr_confmat

library(pROC)
lr_roc = roc(test$Class ~ pred_lr)
plot(lr_roc)
```

### Lasso Regression
```{r}
library(glmnet)
set.seed(11)

x_glm = model.matrix(Class ~ ., new_df)[, -1]

cv_lasso = cv.glmnet(x_glm, new_df$Class, alpha = 1, family = "binomial")
plot(cv_lasso)
lasso = glmnet(x_glm, new_df$Class, alpha = 1, family = "binomial", lambda = cv_lasso$lambda.min)

x_test = model.matrix(Class ~ ., test)[, -1]
pred_lasso = predict(lasso, newx = x_test, type = "response")
lasso_index = as.numeric(pred_lasso > .99)

plot(test_1, test_2, pch = lasso_index+15, col = lasso_index+15)
legend("topleft", legend = c("Fraud", "Non-fraud"), pch = c(15, 16), col = c(15, 16))

lasso_confmat = confusionMatrix(as.factor(lasso_index), as.factor(test$Class), positive = "1")
lasso_confmat

lasso_roc = roc(test$Class ~ pred_lasso)
plot(lasso_roc)
```

### Naive Bayes
```{r}
nb_df = new_df
nb_df$Class = as.factor(new_df$Class)

library(e1071)

nb = naiveBayes(Class ~ ., nb_df, type = "class")

pred_nb = predict(nb, newdata = test, type = "class")

nb_index = as.numeric(pred_nb) -1
plot(test_1, test_2, pch = nb_index+15, col = nb_index+15)
legend("topleft", legend = c("Fraud", "Nonfraud"), pch = nb_index+15, col = nb_index+15)

nb_confmat = confusionMatrix(pred_nb, as.factor(test$Class), positive = "1")
nb_confmat

nb_roc = roc(test$Class, nb_index)
plot(nb_roc)
```

### SVM
```{r}
svm = svm(Class ~ ., nb_df)
plot(svm, nb_df, V1 ~ V2)

pred_svm = predict(svm, test)

svm_index = as.numeric(pred_svm) - 1
plot(test_1, test_2, pch = svm_index+15, col = svm_index+15)
legend("topleft", legend = c("Fraud", "Nonfraud"), pch = c(15, 16), col = c(15, 16))

svm_confmat = confusionMatrix(pred_svm, as.factor(test$Class), positive = "1")
svm_confmat

svm_roc = roc(test$Class, svm_index)
plot(svm_roc)
```

### Decision Tree
```{r}
library(rpart)

dt = rpart(Class ~ ., new_df, method = "class")

pred_dt = predict(dt, newdata = test, type = "class")

dt_index = as.numeric(pred_dt) - 1
plot(test_1, test_2, pch = dt_index+15, col = dt_index+15)
legend("topleft", legend = c("Fraud", "Nonfraud"), pch = c(15, 16), col = c(15, 16))

dt_confmat = confusionMatrix(pred_dt, as.factor(test$Class), positive = "1")
dt_confmat

dt_roc = roc(test$Class, dt_index)
plot(dt_roc)
```

### kNN
```{r}
library(FNN)

knn_perf = data.frame(k = 1:5, acc = 0, sens = 0)
for (i in 1:5) {
  fit = knn(new_df[, -31], test[, -31], new_df[, 31], k = i)
  confmat = confusionMatrix(as.factor(fit), as.factor(test$Class), positive = "1")
  knn_perf[i, ] = c(i, confmat$overall['Accuracy'], confmat$byClass['Sensitivity'])
}
knn_perf
```

k=1 gives best sensitivity, but it has danger of potential overfitting. I chose k=3 for the next best parameter in terms of sensitivity.

```{r}
knn = knn(new_df[, -31], test[, -31], new_df[, 31], k = 3)

knn_index = as.numeric(knn)-1
plot(test_1, test_2, pch = knn_index+15, col = knn_index+15)
legend("topleft", legend = c("Fraud", "Nonfraud"), pch = c(15, 16), col = c(15, 16))

knn_confmat = confusionMatrix(knn, as.factor(test$Class), positive = "1")
knn_confmat

knn_roc = roc(test$Class, knn_index)
plot(knn_roc)
```

### Random Forest
```{r}
library(randomForest)

rf = randomForest(Class ~ ., nb_df)
pred_rf = predict(rf, test, type = "class")

rf_index = as.numeric(pred_rf)-1
plot(test_1, test_2, pch = rf_index+15, col = rf_index+15)
legend("topleft", legend = c("Fraud", "Nonfraud"), pch = c(15, 16), col = c(15, 16))

rf_confmat = confusionMatrix(pred_rf, as.factor(test$Class), positive = "1")
rf_confmat

rf_roc = roc(test$Class ~ rf_index)
plot(rf_roc)
```

### Gradient Boosting
```{r}
library(gbm)

gbm = gbm(Class ~ ., data = new_df, distribution = "bernoulli")
pred_gbm = predict(gbm, test, n.trees = 100, type = "response")
gbm_index = as.numeric(pred_gbm > .9)

plot(test_1, test_2, pch = gbm_index+15, col = gbm_index+15)
legend("topleft", legend = c("Fraud", "Nonfraud"), pch = c(15, 16), col = c(15, 16))

gbm_confmat = confusionMatrix(as.factor(gbm_index), as.factor(test$Class), positive = "1")
gbm_confmat

gbm_roc = roc(test$Class ~ gbm_index)
plot(rf_roc)
```

## Undersample summary
```{r}
legends = c("Logistic Regression", "LASSO Regression", "Naive Bayes", "Support Vector Machine",
            "Decision Tree", "kNN", "Random Forest", "Gredient Boosting")
auc = round(c(lr_roc$auc, lasso_roc$auc, nb_roc$auc, svm_roc$auc, dt_roc$auc, knn_roc$auc,
              rf_roc$auc, gbm_roc$auc), 4)
legends = paste(legends, rep("(", 8), auc, rep(")", 8), sep = "")

# roc curves
plot(lr_roc, col = 1)
lines(lasso_roc, col = 2)
lines(nb_roc, col = 3)
lines(svm_roc, col = 4)
lines(dt_roc, col = 5)
lines(knn_roc, col = 6)
lines(rf_roc, col = 7)
lines(gbm_roc, col = 8)
legend("bottomright", legend = legends, col = 1:8, lty = "solid", cex = .8)
```

Let's move on with logistic regression and random forest, with the two best performance.


# SMOTE: Oversampling
```{r}
library(DMwR)
train$Class = as.factor(train$Class)

smote_df = SMOTE(Class ~ ., train, perc.over = 1000, perc.under = 300, k = 5)
table(smote_df$Class)
```
By oversampling, we can fully exploit the information without loss. This method will result in better accuracy than random under-sampling.

```{r}
lr_smote = glm(Class ~ ., smote_df, family = "binomial")

pred_lr_smote = predict(lr_smote, newdata = test, type = "response")
lr_smote_index = as.numeric(pred_lr_smote > 0.9) # cutoff

plot(test_1, test_2, pch = lr_smote_index+15, col = lr_smote_index+15)
legend("topleft", legend = c("Fraud", "Non-fraud"), pch = c(15, 16), col = c(15, 16))

lr_smote_confmat = confusionMatrix(as.factor(lr_smote_index), as.factor(test$Class), positive = "1")
lr_smote_confmat

lr_smote_roc = roc(test$Class ~ pred_lr_smote)
plot(lr_smote_roc)
```

# Total summary
```{r}
legends = c("Logistic Regression", "SMOTE", "LASSO Regression", "Naive Bayes", "Support Vector Machine",
            "Decision Tree", "kNN", "Random Forest", "Gredient Boosting")
auc = round(c(lr_roc$auc, lr_smote_roc$auc, lasso_roc$auc, nb_roc$auc, svm_roc$auc, 
              dt_roc$auc, knn_roc$auc, rf_roc$auc, gbm_roc$auc), 4)
legends_auc = paste(legends, rep("(", 9), auc, rep(")", 9), sep = "")

# roc curves
plot(lr_roc, col = 1)
lines(lr_smote_roc, col = 2)
lines(lasso_roc, col = 3)
lines(nb_roc, col = 4)
lines(svm_roc, col = 5)
lines(dt_roc, col = 6)
lines(knn_roc, col = 7)
lines(rf_roc, col = 8)
lines(gbm_roc, col = 9)
legend("bottomright", legend = legends_auc, col = 1:9, lty = "solid", cex = .8)

# performance
perf = data.frame(Method = legends,
                  Accuracy = c(lr_confmat$overall['Accuracy'], lr_smote_confmat$overall['Accuracy'],
                               lasso_confmat$overall['Accuracy'], nb_confmat$overall['Accuracy'],
                               svm_confmat$overall['Accuracy'], dt_confmat$overall['Accuracy'],
                               knn_confmat$overall['Accuracy'], rf_confmat$overall['Accuracy'],
                               gbm_confmat$overall['Accuracy']),
                  Sensitivity = c(lr_confmat$byClass['Sensitivity'], lr_smote_confmat$byClass['Sensitivity'],
                                  lasso_confmat$byClass['Sensitivity'], nb_confmat$byClass['Sensitivity'],
                                  svm_confmat$byClass['Sensitivity'], dt_confmat$byClass['Sensitivity'],
                                  knn_confmat$byClass['Sensitivity'], rf_confmat$byClass['Sensitivity'],
                                  gbm_confmat$byClass['Sensitivity']),
                  Specificity = c(lr_confmat$byClass['Specificity'], lr_smote_confmat$byClass['Specificity'],
                                  lasso_confmat$byClass['Specificity'], nb_confmat$byClass['Specificity'],
                                  svm_confmat$byClass['Specificity'], dt_confmat$byClass['Specificity'],
                                  knn_confmat$byClass['Specificity'], rf_confmat$byClass['Specificity'],
                                  gbm_confmat$byClass['Specificity']),
                  AUC = c(lr_roc$auc, lr_smote_roc$auc, lasso_roc$auc, nb_roc$auc, svm_roc$auc,
                          dt_roc$auc, knn_roc$auc, rf_roc$auc, gbm_roc$auc)
                  )
perf
```
Surprisingly, SMOTE performed worse than simple logistic regression in terms of sensitivity and AUC, whereas accuracy is better. This maybe is due to the failure to optimize parameters.  
Overall, LASSO performed the best in terms of accuracy, random forest in terms of sensitivity, LASSO regression in terms of specificity, and random forest in terms of AUC. Considering that wrongly classifying a nonfraud as a fraud and enforcing measures may cause bigger hassle and costs than wrongly classifying a fraud as nonfraud, we can say specificity is more important here. Then, LASSO regression seems to be the model we are looking for, although random forest may come in handy if we really want to catch the frauds.
